{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47c84d6f",
   "metadata": {},
   "source": [
    "#  Python for Economic and Social Data Science: Lecture Five\n",
    "\n",
    "---\n",
    "\n",
    "Lets now run the homework randomiser again, where each randomly selected student will answer some of the homework questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251fb7b0",
   "metadata": {},
   "source": [
    "## 16 Matplotlib\n",
    "\n",
    "The attractiveness of Matplotlib lies in the fact that it is widely considered to be a perfect alternative to\n",
    "MATLAB, if it is used in combination with Numpy and Scipy. Whereas MATLAB is expensive and closed source, Matplotlib is free and open source code. It is also object-oriented and can be used in an object oriented way. Matplotlib can be used to create publication quality figures in a variety of hardcopy formats and interactive environments across platforms.\n",
    "\n",
    "The key message of the Matplotlib website states:\n",
    "\n",
    "```\n",
    "matplotlib tries to make easy things easy and hard things possible\n",
    "```\n",
    "\n",
    "We will use the `pyplot` submodule of `matplotlib`. `pyplot` provides a procedural interface to the object-oriented plotting library of matplotlib. Its plotting commands are chosen in a way that they are similar to Matlab both in naming and with the arguments.\n",
    "\n",
    "### 16.1 Basic Examples\n",
    "\n",
    "Lets see our first basic example of how to use matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a303a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04927266",
   "metadata": {},
   "source": [
    "And then, our first plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcede5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([5, 7, 10, 15, 18, 20, 25, 30, 40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3959a5f8",
   "metadata": {},
   "source": [
    "The graph is continuous, but we might want red markers for this discrete data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f568783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([5, 7, 10, 15, 18, 20, 25, 30, 40], '.r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214c2c9d",
   "metadata": {},
   "source": [
    "### 16.2 Two main approaches\n",
    "\n",
    "#### 16.2.1 Using pyplot directly\n",
    "\n",
    "There are two main approaches to creating plots in Matplotlib:\n",
    "\n",
    "Using the pyplot module (plt): This is the state-based interface to Matplotlib and works similarly to MATLAB. It keeps track of the current figure and axes, making it very simple to create plots quickly. This approach involves calling functions directly from the pyplot module. It's straightforward and concise, making it ideal for simple and quick visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = [0, 1, 4, 9, 16]\n",
    "y2 = [0, 1, 2, 3, 4]\n",
    "plt.plot(y1, label='Quadratic')\n",
    "plt.plot(y2, label='Linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3830f8df",
   "metadata": {},
   "source": [
    "#### 16.2.2 The object orientated approach\n",
    "\n",
    "The other way is to use the object orientated approach: (fig, ax). This approach provides more control and customization by explicitly creating figure and axes objects. It is particularly useful when you want to create more complex or multi-plot figures. \n",
    "\n",
    "This method involves explicitly creating figure and axes objects, which allows for more customization and control, especially useful for more complex plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668870bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax1.plot(y1)\n",
    "ax1.plot(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d32eb29",
   "metadata": {},
   "source": [
    "A couple of things to note here: note how the subplots allows us to specify the number of subfigures in our plot, and also note how it allows us to vary the figsize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4368652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "ax1.plot(y1)\n",
    "ax2.plot(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf996f8",
   "metadata": {},
   "source": [
    "The top of the tree-like structure of matplotlib objects is the figure object. A figure can be seen as the container which contains one or more plots. The plots are called axes in matplotlib jargon. Axes is the plural of the word \"axis\", which gives us a misleading idea. We can think about \"axes\" as a synonym for the word \"plot\"\n",
    "\n",
    "In general, we are going to stick to this more object orientated approach, as it's our goal to create more complex figures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232aaaf8",
   "metadata": {},
   "source": [
    "### 16.2 Plot types\n",
    "\n",
    "Lets now introduce some other very basic plot types, before we move on to styling and more advanced plot types.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac88974b",
   "metadata": {},
   "source": [
    "#### 16.2.1 Line Graphs\n",
    "\n",
    "Well, we've already seen the default plotting behaviour of matplotlib above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9e8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1, 1, figsize=(5, 3))\n",
    "ax1.plot([0, 1, 4, 9, 16])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9650801",
   "metadata": {},
   "source": [
    "#### 16.2.2 Scatter Graphs\n",
    "\n",
    "And for a scatter plot with markers (can you identify how similar this is to plotting a line graph, conceptually?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b16b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1, 1, figsize=(5, 3))\n",
    "ax1.plot([0, 1, 4, 9, 16], marker='o', color='red') # what happens if we specify \"marker='o'?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb2b1fe",
   "metadata": {},
   "source": [
    "#### 16.2.3 Bar charts\n",
    "\n",
    "A very simple example of plotting bar charts follows below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b403d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note how we conventionally declare this first\n",
    "fig, (ax1) = plt.subplots(1, 1, figsize=(5, 3))\n",
    "years = [str(year) for year in range(2018, 2024)]\n",
    "visitors = [123, 156, 300, 350, 425, 450]\n",
    "plt.bar(years, visitors, color=\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e484a2",
   "metadata": {},
   "source": [
    "#### 16.2.3 Histograms\n",
    "\n",
    "Lets use some of our numpy skills gained in Lecture_3.ipynb to plot a standard Gausian distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47d7324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fig, (ax1) = plt.subplots(1, 1, figsize=(5, 3))\n",
    "gaussian_numbers = np.random.normal(size=1000000)\n",
    "ax1.hist(gaussian_numbers, bins=20, color='blue');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29af4866",
   "metadata": {},
   "source": [
    "Note how we've added in an extra couple of parametres here. Lets keep going along that theme!\n",
    "\n",
    "#### 16.2.4\n",
    "\n",
    "Lets now see an example of 'stacked' plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b7addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1, 1, figsize=(5, 3))\n",
    "y1 = [23, 42, 33, 43, 8, 44, 43, 18, 21]\n",
    "y2 = [9, 31, 25, 14, 17, 17, 42, 22, 28]\n",
    "y3 = [18, 29, 19, 22, 18, 16, 13, 32, 21]\n",
    "idxes = [ 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "ax1.stackplot(idxes, y1, y2, y3); # What does the semi colon do both above and here?\n",
    "# What happens here if we _dont_ declare the x-axis values?\n",
    "# Note: does the ax.stackplot() method always expect x-first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da955df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots() # Why would we not want to declare figsize there?\n",
    "labels = 'C', 'Python', 'Java', 'C++', 'C#'\n",
    "sizes = [13.38, 11.87, 11.74, 7.81, 4.41]\n",
    "explode = (0, 0.1, 0, 0, 0)\n",
    "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35817129",
   "metadata": {},
   "source": [
    "### 16.3 Combining Object Orientated plot types\n",
    "\n",
    "Lets walk now try to stack four of the aforementioned plot types together into one figure, using four different axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bac3169",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(10, 6))\n",
    "ax1.plot([0, 1, 4, 9, 16])\n",
    "ax2.plot([0, 1, 4, 9, 16], 'o')\n",
    "ax3.hist(gaussian_numbers, bins=20, color='blue')\n",
    "ax4.stackplot(idxes, y1, y2, y3); # why do we only need the semi colon here and not above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39295990",
   "metadata": {},
   "source": [
    "### 16.4 A comprehensive example\n",
    "\n",
    "Lets now see a comprehensive worked example from a famous dataset, which we can then use towards advanced plotting tools (and into our ```statsmodels``` and ```robustipy``` examples.\n",
    "\n",
    "#### 16.4.1 Getting data and preprocessing it\n",
    "\n",
    "First, lets re-cap all of the things we learnt previously about requests and pandas. We're going to do this with a 'real world' dataset; the `nlsw88.dta` file from Stata.\n",
    "\n",
    "The `nlsw88.dta` dataset is another sample dataset frequently used for educational purposes in Stata. This dataset comes from the National Longitudinal Survey of Young Women, specifically the 1988 survey (hence the \"88\" in the name). It contains data on the labor market experiences of young women in the United States.\n",
    "\n",
    "Here are some of the key variables that you might find in the `nlsw88.dta` dataset:\n",
    "\n",
    "* `idcode`: A unique identifier for each individual in the dataset.\n",
    "* `age`: The age of the respondent.\n",
    "* `race`: The race of the respondent.\n",
    "* `msp`: Marital status of the respondent (1 = married, 0 = not married).\n",
    "* `nevmar`: Never married (1 = never married, 0 = ever married).\n",
    "* `grade`: Highest grade completed by the respondent.\n",
    "* `collgrad`: College graduate status (1 = college graduate, 0 = not a college graduate).\n",
    "* `south`: Whether the respondent lives in the southern United States (1 = yes, 0 = no).\n",
    "* `smsa`: Whether the respondent lives in a metropolitan area (1 = yes, 0 = no).\n",
    "* `hours`: Usual hours worked per week.\n",
    "* `ttl_exp`: Total work experience in years.\n",
    "* `tenure`: Job tenure in years.\n",
    "* `union`: Union membership status (1 = union member, 0 = not a union member).\n",
    "* `wage`: Hourly wage rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_union(path_to_union):\n",
    "    \"\"\"\n",
    "    Prepare data for union example.\n",
    "\n",
    "    Reads a Stata file from the given path, processes the data, and prepares it for\n",
    "    regression analysis. The function creates binary indicators for categorical variables,\n",
    "    augments the input data with a log-transformed wage variable, and handles missing\n",
    "    values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_to_union : str\n",
    "        File path to the Stata file containing union data.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    tuple: A tuple containing the dependent variable ('y'), list of control variables ('c'),\n",
    "           independent variable ('x'), and the prepared DataFrame ('final_data').\n",
    "\n",
    "    Raises\n",
    "    ----------\n",
    "    FileNotFoundError: If the file specified in 'path_to_union' does not exist.\n",
    "    \"\"\"\n",
    "    union_df = pd.read_stata(path_to_union)\n",
    "    union_df[['smsa','collgrad','married','union']]= union_df[['smsa',\n",
    "                                                               'collgrad',\n",
    "                                                               'married',\n",
    "                                                               'union']].astype('str')\n",
    "    union_df.loc[:, 'log_wage'] = np.log(union_df['wage'].copy()) * 100\n",
    "    union_df = union_df[union_df['union'].notnull()].copy()\n",
    "    union_df.loc[:, 'union'] = np.where(union_df['union'] == 'union', 1, 0)\n",
    "    union_df.loc[:, 'married'] = np.where(union_df['married'] == 'married', 1, 0)\n",
    "    union_df.loc[:, 'collgrad'] = np.where(union_df['collgrad'] == 'college grad', 1, 0)\n",
    "    union_df.loc[:, 'smsa'] = np.where(union_df['smsa'] == 'SMSA', 1, 0)\n",
    "    union_df[['smsa', 'collgrad', 'married', 'union']] = union_df[['smsa',\n",
    "                                                                   'collgrad',\n",
    "                                                                   'married',\n",
    "                                                                   'union']].astype('category')\n",
    "    indep_list = ['hours',\n",
    "                  'age',\n",
    "                  'grade',\n",
    "                  'collgrad',\n",
    "                  'married',\n",
    "                  'south',\n",
    "                  'smsa',\n",
    "                  'c_city',\n",
    "                  'ttl_exp',\n",
    "                  'tenure']\n",
    "    for var in indep_list:\n",
    "        union_df = union_df[union_df[var].notnull()]\n",
    "    y = 'log_wage'\n",
    "    c = indep_list\n",
    "    x = 'union'\n",
    "    final_data = pd.merge(union_df[y],\n",
    "                          union_df[x],\n",
    "                          how='left',\n",
    "                          left_index=True,\n",
    "                          right_index=True)\n",
    "    final_data = pd.merge(final_data,\n",
    "                          union_df[indep_list],\n",
    "                          how='left',\n",
    "                          left_index=True,\n",
    "                          right_index=True)\n",
    "    final_data = final_data.reset_index(drop=True)\n",
    "    return final_data\n",
    "\n",
    "\n",
    "def union_example():\n",
    "    \"\"\" A simple function for estimating the wage premium or\n",
    "        penalty as a function of the NLSW1988 dataset\n",
    "\n",
    "        Inputs: None\n",
    "        Outputs: dependent, control, and independent variable of interest\n",
    "               : (otherwise known as y, c, and x)\n",
    "               \n",
    "    \"\"\"\n",
    "    \n",
    "    # First, lets grab our dataset using the requests library:\n",
    "    \n",
    "    def get_nlsw88(url, path, fname):\n",
    "        url = 'https://www.stata-press.com/data/r10/nlsw88.dta'\n",
    "        response = requests.get(url, stream=True)\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "            print(f\"Directory '{path}' and all required subdirectories created.\")\n",
    "        else:\n",
    "            print(f\"Directory '{path}' already exists.\")\n",
    "   \n",
    "        if response.status_code == 200:\n",
    "            with open(os.path.join(path, fname), 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            print(f\"File downloaded successfully and saved as {fname}\")\n",
    "        else:\n",
    "            print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "    \n",
    "    get_nlsw88('https://www.stata-press.com/data/r10/nlsw88.dta',\n",
    "               os.path.join('..', 'Data'), 'nlsw88.dta')\n",
    "    \n",
    "    # Then, lets make a simple function call which prepares this data akin with the best\n",
    "    # practice econometric\\statistical literature.\n",
    "    \n",
    "    data = prepare_union(os.path.join('data',\n",
    "                                      'input',\n",
    "                                      'nlsw88.dta'\n",
    "                                     )\n",
    "                        )\n",
    "    return data\n",
    "\n",
    "data = union_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c438c25",
   "metadata": {},
   "source": [
    "#### 16.4.2 Previewing data\n",
    "\n",
    "What does this data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edde19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by='log_wage', ascending=False)#.head(5)\n",
    "#print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fef7d7",
   "metadata": {},
   "source": [
    "#### 16.4.3 Making a simple matplotlib figure\n",
    "\n",
    "Lets now make a simple 1x2 matplotlib figure from this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3af7e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "data[['log_wage', 'tenure']].plot(y='log_wage', x='tenure', kind='scatter', ax=ax1)\n",
    "\n",
    "data['age'].plot(ax=ax2, kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d99b667",
   "metadata": {},
   "source": [
    "#### 16.4.3 Figure Aesthetics\n",
    "\n",
    "Lets now do everything we can do to make this plot as aesthetically pleasing as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b94c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "colors = ['#001c54', '#E89818', '#8b0000']\n",
    "    \n",
    "data[['log_wage', 'tenure']].plot(y='log_wage',\n",
    "                                  x='tenure',\n",
    "                                  ax=ax1,\n",
    "                                  kind='scatter',\n",
    "                                  s=50,\n",
    "                                  edgecolor=(0, 0, 0, 1), #edgecolor='black'\n",
    "                                  color=colors[1], #facecolor=colors[1]\n",
    "                                 )\n",
    "data['age'].plot(ax=ax2,\n",
    "                 kind='hist',\n",
    "                 edgecolor='k',\n",
    "                 color=colors[0],\n",
    "                 alpha=0.75,\n",
    "                 legend=True)\n",
    "\n",
    "#ax1.set_title('a.', loc='left', fontsize=14)\n",
    "ax1.set_title('This is a formatted scatter plot', fontsize=14, loc='center')\n",
    "ax2.set_title('b.', loc='left', fontsize=14)\n",
    "\n",
    "ax1.grid(which=\"both\", linestyle='-', alpha=1)\n",
    "ax2.grid(which=\"both\", linestyle='--', alpha=0.3)\n",
    "\n",
    "ax1.set_ylabel('Logarithmic Wage', fontsize=22)\n",
    "ax2.set_ylabel('Frequency')\n",
    "\n",
    "ax1.set_xlabel('Tenure')\n",
    "ax2.set_xlabel('Age')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.despine(ax=ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7c1ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nejm = pd.read_csv(os.path.join(os.getcwd(),\n",
    "                                '..',\n",
    "                                'Data',\n",
    "                                'metadata_nejm.csv')\n",
    "                  )\n",
    "print(nejm.head(2))\n",
    "\n",
    "print('*'*20)\n",
    "\n",
    "nature = pd.read_csv(os.path.join(os.getcwd(),\n",
    "                     '..',\n",
    "                     'Data',\n",
    "                     'metadata_nature.csv')\n",
    "                    )\n",
    "print(nature.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ec741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topics_barplot(nejm, nature, figure_path):\n",
    "    \n",
    "    fig, ((ax1, ax2)) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    colors = ['#001c54', '#E89818']\n",
    "    \n",
    "    nbins = 25\n",
    "    \n",
    "    sns.histplot(nejm['topics_count'],\n",
    "                 ax=ax1,\n",
    "                 color=colors[0],\n",
    "                 bins=nbins)\n",
    "    ax1_twin = ax1.twinx()\n",
    "    sns.kdeplot(nejm['topics_count'], ax=ax1_twin, color=colors[1])\n",
    "                       \n",
    "    sns.histplot(nature['topics_count'],\n",
    "                 ax=ax2,\n",
    "                 color=colors[0],\n",
    "                 bins=nbins)\n",
    "    ax2_twin = ax2.twinx()\n",
    "    sns.kdeplot(nature['topics_count'], ax=ax2_twin, color=colors[1])\n",
    "    \n",
    "    ax1.set_title('a.', loc='left', fontsize=23)\n",
    "    ax2.set_title('b.', loc='left', fontsize=23)\n",
    "    ax1.set_xlim(0, ax1.get_xlim()[1])\n",
    "    ax2.set_xlim(0, ax2.get_xlim()[1])\n",
    "    ax1.grid(which=\"both\", linestyle='--', alpha=0.25)\n",
    "    ax2.grid(which=\"both\", linestyle='--', alpha=0.25)\n",
    "    ax1.set_axisbelow(True)\n",
    "    ax2.set_axisbelow(True)\n",
    "    \n",
    "    from matplotlib.lines import Line2D\n",
    "    from matplotlib.patches import Patch\n",
    "    \n",
    "    legend_elements1 = [\n",
    "        Patch(facecolor=colors[0], edgecolor=(0, 0, 0, 1),\n",
    "              label=r'Histogram'),\n",
    "        Line2D([0], [0], color=colors[1], lw=2, linestyle='-',\n",
    "               label=r'Kernel Density', alpha=0.7)\n",
    "    ]\n",
    "\n",
    "    ax1.legend(handles=legend_elements1, loc='upper left', frameon=True,\n",
    "               fontsize=12, framealpha=1, facecolor='w',\n",
    "               edgecolor=(0, 0, 0, 1), ncols=1, title='New England Journal\\n       Of Medicine'\n",
    "               )\n",
    "    ax2.legend(handles=legend_elements1, loc='upper right', frameon=True,\n",
    "               fontsize=12, framealpha=1, facecolor='w',\n",
    "               edgecolor=(0, 0, 0, 1), ncols=1, title='Nature'\n",
    "               )\n",
    "    \n",
    "\n",
    "    print(f\"NEJM mean number of topics: {nejm['topics_count'].mean()}\")\n",
    "    print(f\"NEJM min number of topics: {nejm['topics_count'].min()}\")\n",
    "    print(f\"NEJM max number of topics: {nejm['topics_count'].max()}\")\n",
    "    print(f\"Nature mean number of topics: {nature['topics_count'].mean()}\")\n",
    "    print(f\"Nature min number of topics: {nature['topics_count'].min()}\")\n",
    "    print(f\"Nature max number of topics: {nature['topics_count'].max()}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figure_path + '.pdf')\n",
    "    plt.savefig(figure_path + '.svg')\n",
    "    plt.savefig(figure_path + '.png', dpi=100)\n",
    "\n",
    "    \n",
    "plot_topics_barplot(nejm, nature, '../Figures/topic_modelling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39451294",
   "metadata": {},
   "source": [
    "### 16.5 Colours and figure types\n",
    "\n",
    "I have some major pieces of advice for people who want to begin to plot production quality figures.\n",
    "\n",
    "* **Always** try to produce .pdf or .svg figures which are 'vector graphics' and therefore lossless. Avoid .pngs wherever possible.\n",
    "* **Always** try to find a consistent colour palette for your paper which works on aggregate.\n",
    "* **Always** try to include in your figures as much useful information as possible to make the point which your paper needs to make, without ever either including too much, or too many figures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7932a1",
   "metadata": {},
   "source": [
    "### 16.6 Your Turn!\n",
    "\n",
    "Find another variable in the nlsw88 file that can be visualised. Visualise it, practicing all of the plotting functions we've learnt about above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a65c5a4",
   "metadata": {},
   "source": [
    "## 17. StatsModels\n",
    "\n",
    "Statsmodels is a Python library designed for statistical modeling, hypothesis testing, and data exploration. It provides classes and functions for estimating and testing statistical models, including linear and logistic regression, time series analysis, and generalized linear models. The library offers comprehensive tools for statistical analysis, including support for univariate and multivariate models, descriptive statistics, and visualization capabilities.\n",
    "\n",
    "Statsmodels integrates well with other Python libraries like NumPy, SciPy, and pandas, enabling users to manipulate data efficiently and perform complex analyses. Its API is designed to be user-friendly, allowing easy access to a wide range of statistical tests and model diagnostics. With robust documentation and an active community, Statsmodels is a valuable resource for data scientists, economists, and researchers looking to perform rigorous statistical analyses and build predictive models in Python.\n",
    "\n",
    "Lets use the nlsw88 data to show a key example of it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181ca9d1",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b36fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f3bb3",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9c23e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = union_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98db0896",
   "metadata": {},
   "source": [
    "Display the first few rows of the dataset. This helps us understand the structure and variables in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85153bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889a5d99",
   "metadata": {},
   "source": [
    "### 17.1 OLS Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf29b2c",
   "metadata": {},
   "source": [
    "Define the dependent and independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a228f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['union',\n",
    "          'hours']]\n",
    "y = data['log_wage']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568775aa",
   "metadata": {},
   "source": [
    "Adding a constant allows us to include the intercept in our regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad7d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd47c17",
   "metadata": {},
   "source": [
    "We use the Ordinary Least Squares (OLS) method to fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1997af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, X).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c59bec",
   "metadata": {},
   "source": [
    "The summary provides detailed statistics about the regression results, including coefficients, R-squared, p-values, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d1dd96",
   "metadata": {},
   "source": [
    "more extensions of what i can do with this model.summary variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c5ea90",
   "metadata": {},
   "source": [
    "### 17.2. Residual visualisation\n",
    "\n",
    "We can also assess the validity of our model, with a couple of matplotlib figures. A quick note here is on the importance of satisfying the Gauss-Markov assumptions (which are often violated by many applied econometric models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e7240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Residuals vs Fitted plot\n",
    "sns.residplot(x=model.fittedvalues,\n",
    "              y=model.resid,\n",
    "              lowess=True,\n",
    "              line_kws={'color': 'red', 'lw': 2},\n",
    "              ax=axs[0])\n",
    "axs[0].set_xlabel('Fitted values')\n",
    "axs[0].set_ylabel('Residuals')\n",
    "axs[0].set_title('Residuals vs Fitted')\n",
    "\n",
    "# Q-Q plot\n",
    "sm.qqplot(model.resid, line='45', ax=axs[1])\n",
    "axs[1].set_title('Q-Q plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca0e83c",
   "metadata": {},
   "source": [
    "### 17.3 Prediction of new depedent variable data\n",
    "\n",
    "We can also predict new dependent variable data (e.g. $\\hat{y}$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01a7c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.DataFrame({\n",
    "    'const': [1],\n",
    "    'age': [30],\n",
    "    'hours': [35],\n",
    "    'tenure': [5],\n",
    "    'union': [1]\n",
    "}\n",
    ")\n",
    "predicted_wage = model.predict(new_data)\n",
    "print(\"Predicted wage:\", predicted_wage[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767def66",
   "metadata": {},
   "source": [
    "#### 17.4 Evaluating the model\n",
    "\n",
    "You can evaluate the model performance using different metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd70390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y, model.fittedvalues)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y, model.fittedvalues)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e56f88",
   "metadata": {},
   "source": [
    "#### 17.5 Model Comparison\n",
    "\n",
    "We can also compare models with all our favourite information criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c1a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = data[['union', 'age']]\n",
    "X2 = sm.add_constant(X2)\n",
    "model2 = sm.OLS(y, X2).fit()\n",
    "\n",
    "print(\"AIC of model 1:\", model.aic)\n",
    "print(\"AIC of model 2:\", model2.aic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa70ad7f",
   "metadata": {},
   "source": [
    "### 18. RobustiPy\n",
    "\n",
    "Lets now introduce [RobustiPy!](https://robustipy.github.io/)\n",
    "\n",
    "RobustiPy is an efficient multiversal library with model selection, averaging, resampling and out-of-sample analysis. It analyses various output spaces, in addition to the control variable space (e.g. multiple dependent variables, estimands of interest, etc). Developed for Python and R, it is designed to be both accessible and computationally efficient.\n",
    "\n",
    "Lets install that library as described in the [github page](https://github.com/RobustiPy/robustipy) for it, then run through some of the empirical and simulated examples within the repositories there following a short presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea0b8e7",
   "metadata": {},
   "source": [
    "## 19. Scikitlearn \n",
    "\n",
    "**Overview:**\n",
    "\n",
    "* ```scikit-learn``` is a free, open-source machine learning library for Python. It provides simple and efficient tools for data mining and data analysis. Built on NumPy, SciPy, and matplotlib.\n",
    "\n",
    "**Features:**\n",
    "\n",
    "* *Classification*: Identifying which category an object belongs to (e.g., spam detection).\n",
    "* *Regression:* Predicting a continuous-valued attribute (e.g., stock prices).\n",
    "* *Clustering:* Grouping similar objects into sets (e.g., customer segmentation).\n",
    "* *Dimensionality Reduction:* Reducing the number of random variables (e.g., PCA).\n",
    "* *Model Selection:* Comparing, validating, and choosing parameters and models.\n",
    "* *Preprocessing:* Feature extraction and normalization.\n",
    "\n",
    "**Ease of Use:**\n",
    "\n",
    "Well-documented and easy to use API. Extensive user guide and examples for beginners and advanced users.\n",
    "\n",
    "**Performance:**\n",
    "\n",
    "* Designed to be efficient with high performance on many real-world tasks.\n",
    "* Integration with other scientific Python libraries like NumPy and pandas.\n",
    "\n",
    "**Community:**\n",
    "\n",
    "* Large and active community contributing to continuous improvement.\n",
    "* Frequent updates and new releases with bug fixes and new features.\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "* Used in academia and industry for tasks such as predictive analytics, computer vision, natural language processing, and more.\n",
    "\n",
    "** Integration:\n",
    "\n",
    "Compatible with other machine learning libraries like TensorFlow and Keras for more complex models.\n",
    "Scikit-learn is a versatile and powerful tool widely used in the machine learning community due to its simplicity and effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cda6ac",
   "metadata": {},
   "source": [
    "### 19.1 Frontloading library imports\n",
    "\n",
    "Lets now try to be as professional as possible by `frontloading' library imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340407cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164e40e8",
   "metadata": {},
   "source": [
    "### 19.2 Loading data \n",
    "\n",
    "Lets load a commonly used ML dataset: the `iris` datset created by Fisher. The Iris flower data set or Fisher's Iris data set is a multivariate data set used and made famous by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed383df",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Display the first five rows of the dataset\n",
    "print(\"Features:\\n\", X[:10])\n",
    "print(\"\\nLabels:\\n\", y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af5ad7b",
   "metadata": {},
   "source": [
    "### 19.3 Train and test splits\n",
    "\n",
    "Lets now split the dataset into four components: X_train, X_test, y_train, y_test. This is *extremely* common for ML pipelines.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb847ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e45fce",
   "metadata": {},
   "source": [
    "### 19.4 Standardisation\n",
    "\n",
    "Lets now standardize (subtract the mean, divide by the variance) the features or our `iris` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f83cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b8b2f8",
   "metadata": {},
   "source": [
    "### 19.5 Logistic Regressions\n",
    "\n",
    "Lets now train a logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5af3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression()\n",
    "logreg_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8598484",
   "metadata": {},
   "source": [
    "### 19.6 Predictions\n",
    "\n",
    "Lets now make predictions and evaluate the predictions created by a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196acd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logreg = logreg_model.predict(X_test_scaled)\n",
    "\n",
    "# Lets now evaluate the model\n",
    "accuracy_logreg = accuracy_score(y_test, y_pred_logreg)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_logreg:.2f}\")\n",
    "\n",
    "# And display classification report\n",
    "print(\"\\nLogistic Regression Classification Report:\\n\", classification_report(y_test, y_pred_logreg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc6fa66",
   "metadata": {},
   "source": [
    "### 19.6.1. Querying Predictions\n",
    "\n",
    "Does this look right? If so, why, if not, why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727d79b2",
   "metadata": {},
   "source": [
    "### 19.6.2 Random Forest\n",
    "\n",
    "Lets now train a Random Forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94a69ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5105cbf",
   "metadata": {},
   "source": [
    "Lets now make predictions and evaluate the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e824ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf:.2f}\")\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nRandom Forest Classification Report:\\n\", classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72da53ac",
   "metadata": {},
   "source": [
    "### 19.7 Classifications\n",
    "\n",
    "Lets now only use the first two features for visualization purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db236bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cluster = X[:, :2]\n",
    "\n",
    "# Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "y_kmeans = kmeans.fit_predict(X_cluster)\n",
    "\n",
    "# Plot the clustering results using object-oriented notation\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(X_cluster[:, 0], X_cluster[:, 1], c=y_kmeans, cmap='viridis')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_title('KMeans Clustering of Iris Dataset (First Two Features)')\n",
    "\n",
    "# Add a color bar\n",
    "legend = ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "ax.add_artist(legend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63b2c25",
   "metadata": {},
   "source": [
    "## 20. NLTK\n",
    "\n",
    "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
    "\n",
    "NLTK needs to be installed in your Python environment if it's not already available. This step ensures you have access to NLTK's functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec0872",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078f38c6",
   "metadata": {},
   "source": [
    "Import NLTK to start using its various modules and functions for natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fde0a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa142c84",
   "metadata": {},
   "source": [
    "NLTK provides datasets and models that are essential for its operations. Downloading these resources ensures NLTK functions correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd571c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069f44ab",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking down text into meaningful units, such as words or sentences. NLTK's word_tokenize() function performs this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e66eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"NLTK is the powerful tool for text processing. Its part of Charlie's python for data science class taught in Xi'an in July of 2024\"\n",
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73688036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928cbc2e",
   "metadata": {},
   "source": [
    "Stopwords are common words (e.g., \"the\", \"is\", \"and\") that are often filtered out in text analysis because they carry little meaningful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ded8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stopwords.words('english')]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0965e528",
   "metadata": {},
   "source": [
    "Stemming reduces words to their root or base form. It helps in normalizing words for better analysis by stripping affixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6077fb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in filtered_tokens]\n",
    "stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da91577",
   "metadata": {},
   "source": [
    "Part-of-Speech (POS) tagging labels words in a text with their corresponding part of speech (e.g., noun, verb, adjective)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280ad85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "pos_tagged = pos_tag(filtered_tokens)\n",
    "pos_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56872afc",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) identifies named entities such as names of persons, organizations, or locations in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac20283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk\n",
    "!pip install svgling\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "    \n",
    "ner_tagged = ne_chunk(pos_tagged)\n",
    "ner_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb9f03d",
   "metadata": {},
   "source": [
    "Frequency distribution counts the occurrences of words in a text and is useful for understanding the most common terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef6223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "freq_dist = FreqDist(filtered_tokens)\n",
    "freq_dist.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8299ac8f",
   "metadata": {},
   "source": [
    "A word cloud is a visual representation of word frequency where more frequent words appear larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec5c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(filtered_tokens))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb293a",
   "metadata": {},
   "source": [
    "Sentiment analysis assesses the sentiment expressed in a text, such as positive, negative, or neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10c2be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiment_score = sid.polarity_scores(text)\n",
    "sentiment_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b1f9d",
   "metadata": {},
   "source": [
    "Collocations are pairs or groups of words that often occur together in a text and can provide valuable insights into the relationships between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c507a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "\n",
    "# Create a BigramCollocationFinder from filtered tokens\n",
    "finder = BigramCollocationFinder.from_words(filtered_tokens)\n",
    "\n",
    "# Use likelihood ratio as the association measure and find top collocations\n",
    "collocations = finder.nbest(BigramAssocMeasures.likelihood_ratio, 5)\n",
    "collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b237c3b0",
   "metadata": {},
   "source": [
    "Concordance helps analyze how a word is used in different contexts in a text. In this cell:\n",
    "\n",
    "* `Text(filtered_tokens)` converts filtered_tokens into an NLTK Text object.\n",
    "* `concordance('NLTK', lines=5)` searches for occurrences of the word 'NLTK' in the text and displays 5 lines of context around each occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897cff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.text import Text\n",
    "\n",
    "# Convert filtered tokens to NLTK Text object\n",
    "nltk_text = Text(filtered_tokens)\n",
    "\n",
    "# Find concordance for the word 'NLTK'\n",
    "concordance_results = nltk_text.concordance('NLTK', lines=5)\n",
    "concordance_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2701ce6a",
   "metadata": {},
   "source": [
    "## 21. Your Projects\n",
    "\n",
    "If there is time at the end of the course, I'd like to to guide students in the developments of _their_ projects which have inspired them to join the course. Examples might include structured or unstructured data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbad70b",
   "metadata": {},
   "source": [
    "## 22 Further study (and potentially a followup class!)\n",
    "\n",
    "1. Advanced Python Programming\n",
    "    * Object-oriented programming (OOP) principles\n",
    "    * Decorators and context managers\n",
    "    * Generators and iterators\n",
    "    * Functional programming techniques\n",
    "\n",
    "2. Data Manipulation and Cleaning\n",
    "    * Advanced techniques using pandas\n",
    "    * Efficient data wrangling with Dask and Vaex\n",
    "    * Handling missing data and outliers\n",
    "    * Data transformation and feature engineering\n",
    "\n",
    "3. Statistical Analysis and Hypothesis Testing\n",
    "    * Advanced statistical methods\n",
    "    * Bayesian statistics\n",
    "    * Bootstrapping and resampling methods\n",
    "    * Hypothesis testing and confidence intervals\n",
    "\n",
    "4. Machine Learning Algorithms\n",
    "    * In-depth study of algorithms like SVM, XGBoost, and neural networks\n",
    "    * Ensemble methods (bagging, boosting, stacking)\n",
    "    * Hyperparameter tuning and model optimization\n",
    "    * Handling imbalanced datasets\n",
    "\n",
    "5. Deep Learning\n",
    "    * Neural network architectures (CNNs, RNNs, LSTMs)\n",
    "    * Transfer learning and fine-tuning pre-trained models\n",
    "    * TensorFlow and PyTorch frameworks\n",
    "    * Implementing and training complex models\n",
    "\n",
    "6. Big Data Technologies\n",
    "    * Working with Apache Spark using PySpark\n",
    "    * Scalable data processing\n",
    "    * Integration with big data ecosystems (Hadoop, Kafka)\n",
    "    * Real-time data streaming\n",
    "\n",
    "7. Data Visualization\n",
    "    * Advanced plotting with Matplotlib and Seaborn\n",
    "    * Interactive visualizations with Plotly and Bokeh\n",
    "    * Custom dashboards with Dash or Streamlit\n",
    "    * Visualizing complex data sets and results\n",
    "\n",
    "8. Natural Language Processing (NLP)\n",
    "    * Text preprocessing and vectorization techniques\n",
    "    * Sentiment analysis and topic modeling\n",
    "    * Sequence-to-sequence models and transformers\n",
    "    * Applications of NLP in real-world scenarios\n",
    "\n",
    "9. Model Deployment and Productionization\n",
    "    * Creating RESTful APIs with Flask or FastAPI\n",
    "    * Model deployment on cloud platforms (AWS, GCP, Azure)\n",
    "    * Continuous Integration/Continuous Deployment (CI/CD) pipelines\n",
    "    * Monitoring and maintaining deployed models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
